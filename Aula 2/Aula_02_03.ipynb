{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introdução"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nesta atividade vamos seguir aproximadamente o material do livro-texto da disciplina (Géron, capítulo 2), e o notebook do material de suporte do livro em https://github.com/ageron/handson-ml .\n",
    "\n",
    "Sua missão é prever o valor mediano de imóveis em distritos residenciais da Califórnia, baseado em algumas características.\n",
    "\n",
    "O conjunto de dados vem do censo de 1990 da Califórnia: preços hoje em dia estão radicalmente diferentes!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entendendo o problema"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Antes de começar a trabalhar no problema, tente entender o contexto no qual o problema se insere. Por que desejamos prever os preços medianos dos imóveis? No livro, o autor menciona que neste exemplo fictício estamos desenvolvendo um preditor de preços medianos como um subsistema de um sistema de auxílio à tomada de decisão em investimentos imobiliários. \n",
    "\n",
    "No livro, o autor também menciona que o sistema atual da empresa usa estimativas manuais, com um erro de cerca de 15%. Um sistema de machine learning pode, potencialmente, ser mais barato, mais rápido e mais preciso!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Você consegue pensar em outros motivos, em geral, pelos quais a aplicação de machine learning pode ser vantajosa?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Que tipo de problema temos aqui, do ponto de vista de machine learning:\n",
    "\n",
    "- Aprendizado supervisionado ou não-supervisionado?\n",
    "\n",
    "- Classificação ou regressão?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora, selecione a medida de desempenho do seu sistema de machine learning. É importante escolher a medida de performance antes de trabalhar com o problema, para que essa escolha seja idônea. Se deixarmos a escolha para mais tarde, depois de conhecer nossos dados em detalhe, pode ser que sejamos tentados a escolher a medida de performance que nos pareça mais favorável para exibir um 'bom' desempenho! Isso não é ciência!\n",
    "\n",
    "Ademais, a escolha da medida de desempenho deve refletir as demandas do problema que estamos resolvendo, do ponto de vista do cliente, e não apenas aquilo que mais nos convém como pesquisadores. Claro que, na prática, procuramos encontrar um ponto ideal entre aquilo que o cliente deseja e aquilo que é viável matematicamente e computacionalmente.\n",
    "\n",
    "Neste problema vamos escolher a raiz quadrada do erro médio quadrático como medida de erro (RMSE - Root Mean Squared Error). Sejam $X = (x_1, x_2, \\cdots, x_m)$  e $y = (y_1, y_2, \\cdots, y_m)$ nossos conjuntos de testes, onde \n",
    "\n",
    "- $X$ contém as características dos distritos e\n",
    "\n",
    "- $y$ contém os valores medianos de imóveis nos correspondentes distritos. \n",
    "\n",
    "As observações $x_i$ são feitas de $n$ *features* cada: $x_i \\in \\mathbb{R}^n$. As observações $y_i$ são meramente números reais: $y_i \\in \\mathbb{R}$.\n",
    "\n",
    "Seja também $h(x)$ o nosso modelo preditivo de valores medianos de imóveis, que foi obtido através de ajuste (fit) de modelo à partir das amostras de treinamento.\n",
    "\n",
    "Então RMSE é dado por:\n",
    "\n",
    "$$\\text{RMSE}(X, y, h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m}\\left(h(x_i) - y_i\\right)^{2}}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pergunta: por que elevar ao quadrado? Por que aplicar a raiz quadrada?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R:**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inicializando o código"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Na maioria dos nossos trabalhos vamos usar as bibliotecas:\n",
    "\n",
    "- Pandas: uma biblioteca para análise de dados em Python. https://pandas.pydata.org/\n",
    "\n",
    "- NumPy: computação científica em Python. http://www.numpy.org/\n",
    "\n",
    "- Matplotlib: para gráficos e outras visualizações\n",
    "\n",
    "Atenção: mais adiante vamos acabar inevitavelmente misturando o uso de duas estruturas de armazenamento de dados em tabelas: DataFrames (Pandas) e ndarrays (NumPy), fique atento!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%matplotlib inline\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos fixar a semente do gerador de números aleatórios, para ter reproducibilidade neste notebook. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "RANDOM_SEED = 42\r\n",
    "np.random.seed(RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Carregando dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "No livro o autor descreve a necessidade da automação no processo de adquirir e armazenar dados. Isso é muito importante sim! Para simplificar o trabalho de hoje vamos pular esta etapa, e carregar diretamente o arquivo de dados fornecido pelo professor, mas recomendo a leitura e reflexão acerca do material do livro. Afinal, são dicas valiosas de quem esteve 'nas trincheiras', e eu assino embaixo das dicas dele.\n",
    "\n",
    "Por ora vamos apenas carregar o arquivo CSV:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "HOUSING_FILE = 'housing.csv'\r\n",
    "\r\n",
    "\r\n",
    "def load_housing_data(housing_file=HOUSING_FILE):\r\n",
    "    return pd.read_csv(housing_file)\r\n",
    "\r\n",
    "\r\n",
    "housing = load_housing_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A função retorna um DataFrame do Pandas:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type(housing)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um DataFrame é uma tabela com linhas e colunas nomeadas (opcionalmente), e contém vários métodos para indexação, slicing, estatísticas, e muito mais. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entendendo inicialmente os dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cada linha da tabela representa um distrito da Califórnia. Vamos conhecer mais sobre os dados:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing.info()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing.head(n=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perguntas:\n",
    "\n",
    "- Quantos distritos existem na base?\n",
    "\n",
    "20640\n",
    "\n",
    "- Qual é a coluna de valores dependentes?\n",
    "\n",
    "median_house_value\n",
    "\n",
    "- Tem valor faltante?\n",
    "\n",
    "Oui \n",
    "\n",
    "- O que representam as colunas 'total_rooms', 'total_bedrooms', e 'households'?\n",
    "\n",
    "total_rooms = Número total de comodos (incluindo quartos)\n",
    "total_bedrooms = Número total de quartos\n",
    "households = Número de casas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing[housing.isnull().any(axis=1)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing[housing[\"total_rooms\"]<housing[\"total_bedrooms\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para ganhar um entendimento melhor da distribuição dos dados:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note que a coluna ``ocean_proximity`` não aparece aqui, pois trata-se de uma coluna não-numérica. Apenas as colunas numéricas estão presentes, e a coluna ``ocean_proximity`` indica rótulos - trata-se de dados categóricos. \n",
    "\n",
    "Para analisar colunas com dados categóricos, podemos usar o método ``value_counts()`` na *série* associada à coluna de interesse:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing['ocean_proximity'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "type(housing['ocean_proximity'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se as estatisticas descritivas sumárias do método ``describe()`` não são suficientes para se obter uma visão dos dados, tente histogramas:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perguntas:\n",
    "\n",
    "- Notou alguma anomalia? Qual? Por que você acha que isso aconteceu?\n",
    "\n",
    "- Quais são as unidades de medida de cada coluna?\n",
    "\n",
    "- Este dataset tem um problema sério: o preço mediano dos imóveis foi artificialmente limitado em 500k. Talvez no formulário de pesquisa de dados tivesse uma opção, na seção 'Preço do imóvel', onde estava escrito 'mais de 500k'. Será que o sistema vai funcionar para prever o preço mediano de imóveis em bairros muito ricos?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R:**\n",
    "Observamos que a idade máxima das casas não passa de 52, onde há pico. Provavelmente devido a limitações da coleta de dados (52 ou mais anos)\n",
    "Algo semelhante parece acontecer com o valor mediano."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "***PARE!***\n",
    "\n",
    "Antes de prosseguir, devemos dividir nossos dados em duas partes: conjunto de treinamento e de testes! *E devemos jurar solenemente que jamais vamos explorar o conjunto de testes, devemos utilizá-lo apenas para as medidas finais de desempenho do nosso sistema!*\n",
    "\n",
    "Tal medida se faz necessária para evitar overfitting, mesmo sem querer! Pode ser que observemos algum padrão nos dados de teste e incorporemos este padrão (inconscientemente) nas nossas decisões de projeto do modelo de machine learning. Então nosso resultado final será otimista demais, e nosso sistema não terá um desempenho tão bom em produção. À esse tipo de viés induzido por ter 'espiado' os dados de teste chamamos ****data snooping bias****"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dividindo os dados em conjunto de treinamento e de testes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para selecionar um conjunto de testes, basta escolher aleatoriamente algumas amostras do conjunto original. Tipicamente selecionamos 20% da amostra para testes. Os demais pontos de dados serão o conjunto de treinamento.\n",
    "\n",
    "Isto pode ser feito com Scikit-Learn usando a função ``train_test_split()``:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(\n",
    "    housing,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_set.corr()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'{len(train_set)} train + {len(test_set)} test = '\n",
    "      f'{len(train_set) + len(test_set)} amostras')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train = train_set.drop(columns=['median_house_value'])\r\n",
    "y_train = train_set['median_house_value']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalmente isso basta.\n",
    "\n",
    "Porém, o livro tece várias considerações de natureza prática a respeito de como fazer uma boa separação entre conjuntos de treinamento e teste. Em particular:\n",
    "\n",
    "- Separação por valor de hash: quando estamos em um ambiente onde o conjunto de dados cresce a cada dia (como no caso do autor do livro, que era do time de machine learning do YouTube), é importante ter uma política de seleção de dados de teste que não permita que itens de teste migrem para o conjunto de treinamento ao se fazer uma nova amostragem dos dados.\n",
    "\n",
    "- Separação estratificada: quando é importante garantir representatividade proporcional aproximadamente igual nos conjuntos de treinamento e teste, em relação a algum atributo, devemos usar separação estratificada. Por exemplo: se queremos dividir um grupo de pacientes em um experimento entre conjuntos de controle e de tratamento, devemos usar separação estratificada em relação ao sexo dos participantes.\n",
    "\n",
    "O código abaixo faz separação estratificada em relação à uma variável categórica inventada que se relaciona com a renda média:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Constroi uma coluna nova com categorias de renda fictícias.\r\n",
    "housing['income_cat'] = np.ceil(housing['median_income'] / 1.5)\r\n",
    "housing['income_cat'].where(housing['income_cat'] < 5, 5.0, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing['income_cat'].value_counts(True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Divide, de modo estratificado, o conjunto de dados.\r\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\r\n",
    "\r\n",
    "split = StratifiedShuffleSplit(\r\n",
    "    n_splits=1,\r\n",
    "    test_size=0.2,\r\n",
    "    random_state=RANDOM_SEED,\r\n",
    ")\r\n",
    "for train_index, test_index in split.split(housing, housing['income_cat']):\r\n",
    "    strat_train_set = housing.loc[train_index]\r\n",
    "    strat_test_set = housing.loc[test_index]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strat_train_set['income_cat'].value_counts() / len(strat_train_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(strat_train_set['income_cat'].value_counts() / len(strat_train_set)).sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strat_test_set['income_cat'].value_counts() / len(strat_test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove a coluna nova, que foi adicionada apenas temporariamente.\r\n",
    "strat_train_set.drop(['income_cat'], axis=1, inplace=True)\r\n",
    "strat_test_set.drop(['income_cat'], axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strat_train_set.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strat_test_set.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A partir de agora usaremos os conjuntos strat_train_set e strat_test_set. O conjunto strat_train_set será usado para várias explorações e para construir nosso modelo preditivo. O conjunto strat_test_set será usado só no final do projeto, para avaliar a performance final do nosso modelo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explorando melhor os dados de treinamento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nossa missão agora é construir um modelo preditivo à partir dos exemplos do conjunto de treinamento. Para isso, devemos:\n",
    "\n",
    "- Visualizar e explorar os dados para entendê-los melhor\n",
    "\n",
    "- Preparar os dados para machine learning\n",
    "\n",
    "- Escolher uma boa família de modelos\n",
    "\n",
    "- Treinar os modelos, fazer ajuste fino dos hiperparâmetros.\n",
    "\n",
    "Para explorar os dados é recomendável que você separe um pedaço do conjunto de treinamento para explorar. O motivo é prático: fica mais rápido iterar sobre os dados, visualizar, etc! Como este dataset é minúsculo (para os padrões de machine learning), vamos explorar o conjunto de treinamento inteiro mesmo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing = strat_train_set.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dados geográficos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos visualizar os dados geográficos, para começar:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing.plot(kind='scatter', x='longitude', y='latitude')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como temos muitos dados agrupados, fica difícil distinguir a real densidade de pontos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing.plot(\r\n",
    "    kind='scatter',\r\n",
    "    x='longitude',\r\n",
    "    y='latitude',\r\n",
    "    alpha=0.1,\r\n",
    ")  # Teste vários valores de alpha.\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Opa, agora melhorou!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing.plot(\r\n",
    "    kind='scatter',\r\n",
    "    x='longitude',\r\n",
    "    y='latitude',\r\n",
    "    s=housing['population'] / 100,\r\n",
    "    c='median_house_value',\r\n",
    "    cmap=plt.get_cmap('jet'),\r\n",
    "    colorbar=True,\r\n",
    "    label='population',\r\n",
    "    figsize=(10, 7),\r\n",
    "    alpha=0.4,\r\n",
    "    sharex=False,\r\n",
    ")\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Muito melhor! Agora parece que a localização do distrito impacta bastante o preço mediano! Nossa intuição já dizia que esta informação era valiosa, a visualização ajuda a confirmar."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlação entre variáveis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A simples correlaçao linear (correlação de Pearson, ou valor R) entre a variável dependente e cada uma das variáveis independentes já pode ser um bom indicador da chance de sucesso do nosso futuro modelo preditivo.\n",
    "\n",
    "Já a correlação entre variáveis independentes pode indicar que temos variáveis 'repetitivas' no nosso conjunto de dados, e isso pode diminuir o desempenho do nosso modelo preditivo.\n",
    "\n",
    "Observe que estamos olhando apenas para correlação linear, o tipo mais simples de correlação. Para medir relacionamentos não-lineares, existem várias outras medidas de correlação não-linear:\n",
    "\n",
    "- Correlação de postos (rank correlation): correlação de Spearman, correlação $\\tau$ de Kendall. Ambos estão implementados no método ``corr()`` dos DataFrames do Pandas (o default é Pearson).\n",
    "\n",
    "- Informação mútua e outras medidas de teoria da informação\n",
    "\n",
    "- entre outras!\n",
    "\n",
    "Trata-se de uma área fértil da estatística."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pergunta (para casa):\n",
    "\n",
    "- O que são as correlações de Spearman e Kendall?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correlation_matrix = housing.corr()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correlation_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing.corr(method='spearman')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing.corr(method='kendall')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perguntas:\n",
    "\n",
    "1. Observe que a diagonal da matrix de correlação contém apenas valores 1.0. Por que?\n",
    "\n",
    "2. Explique o significado das seguintes correlações:\n",
    "\n",
    "    2.1. 'population' vs 'households'\n",
    "    \n",
    "    2.2. 'households' vs 'total_rooms' e 'households' vs 'total_bedrooms'\n",
    "    \n",
    "    2.3. 'median_house_value' vs 'median_income'\n",
    "    \n",
    "    2.4. 'population' vs 'median_income'\n",
    "    \n",
    "    2.5. 'latitude' vs 'longitude'. Seria similar no Chile? E na cidade de São Paulo? E no Brasil como um todo?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R:**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, mas o que a gente quer é saber: quais variáveis independentes podem nos ajudar a prever a variável dependente? Vamos nos concentrar na coluna 'median_house_value':"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correlation_matrix['median_house_value'].sort_values(ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Em resumo: como dizemos na gíria de ciência dos dados - parece que tem sinal ai!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparando os dados para o modelo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Estamos agora mais encorajados a construir um modelo de machine learning para nosso problema: nossa investigação mostrou a existência de sinal, e não apenas ruído.\n",
    "\n",
    "Temos agora que preparar nossos dados e nosso modelo:\n",
    "\n",
    "- Separar a variável dependente das variáveis independentes\n",
    "\n",
    "- Resolver o problema dos valores faltantes na coluna 'total_bedroom'\n",
    "\n",
    "- A maioria dos modelos de machine learning lida com variáveis numéricas apenas. Temos que fazer algo em relação à variável categórica 'ocean_proximity'\n",
    "\n",
    "- Adicionar outras transformações: criar características extras que podem ser úteis, reescalar características, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Separando X e y"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Variáveis independentes: dataset original menos a coluna de valores dependentes.\r\n",
    "housing = strat_train_set.drop('median_house_value', axis=1)\r\n",
    "\r\n",
    "# Variável dependente, também chamada de label.\r\n",
    "housing_labels = strat_train_set['median_house_value'].copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = housing.copy()\r\n",
    "df['median_house_value'] = housing_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.boxplot(column=['median_house_value'], by='ocean_proximity')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Resolvendo o problema dos valores faltantes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como percebemos antes, estão faltando alguns valores na coluna 'total_bedrooms':"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\r\n",
    "sample_incomplete_rows"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bom, temos 3 alternativas:\n",
    "\n",
    "- Remover a coluna inteira de dados faltantes, ou\n",
    "\n",
    "- Remover as linhas onde estão faltando dados, ou\n",
    "\n",
    "- Preencher os buracos\n",
    "\n",
    "Vamos adotar esta última estratégia. (As outras duas ficam como exercício para vocês.) Qual o valor ideal para usar aqui? Novamente, temos algumas opções:\n",
    "\n",
    "- Preenche com zeros\n",
    "\n",
    "- Treinar um modelo de machine learning para prever os valores desta coluna e usar o modelo para preencher os espaços! Trata-se de uma forma mais sofisticada de interpolação.\n",
    "\n",
    "- Usar alguma estatística do dataset, como a mediana\n",
    "\n",
    "O mais simples e realista é adotar a mediana, é o que vamos fazer então. Scikit-Learn tem uma classe especial de transformadores de dados que serve exatamente para isso, chamada ``SimpleImputer``:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.impute import SimpleImputer\r\n",
    "\r\n",
    "# Antes de treinar o SimpleImputer, remover a coluna de dados categóricos. O dataset resultante tem apenas\r\n",
    "# as variáveis independentes numéricas.\r\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\r\n",
    "\r\n",
    "# Cria um imputer que substitui células inválidas (NaN) pela mediana dos valores da coluna à qual a célula pertence.\r\n",
    "imputer = SimpleImputer(strategy='median')\r\n",
    "\r\n",
    "# Agora treinar o Imputer. Isto vai causar o cálculo da mediana de cada coluna,\r\n",
    "# que ficará armazenado no Imputer para uso futuro.\r\n",
    "imputer.fit(housing_num)\r\n",
    "\r\n",
    "# O Imputer agora tem as estatísticas desejadas armazenadas.\r\n",
    "print('Estatísticas do Imputer:')\r\n",
    "print(imputer.statistics_)\r\n",
    "\r\n",
    "# Compare com as medianas do DataFrame:\r\n",
    "print('Medianas')\r\n",
    "print(housing_num.median().values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora que temos o Imputer para preencher os buracos, vamos usá-lo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Aplicar o Imputer aos nossos dados. O valor de retorno é um ndarray do NumPy.\r\n",
    "temp = imputer.transform(housing_num)\r\n",
    "print(type(temp))\r\n",
    "\r\n",
    "# Trabalhar com DataFrames geralmente é mais legal - dá para referenciar\r\n",
    "# colunas por nome, ao invés de indices. Vamos transformar de volta o ndarray\r\n",
    "# em DataFrame.\r\n",
    "housing_tr = pd.DataFrame(temp, columns=housing_num.columns)\r\n",
    "print(type(housing_tr))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verificando se os buracos foram preenchidos:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing_tr[housing_tr.isnull().any(axis=1)].head()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ótimo, não tem mais buraco!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Codificando variáveis categóricas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A maioria dos algoritmos de machine learning trabalha apenas com variáveis numéricas, mas a coluna 'ocean_proximity' tem strings, representando categorias. Como proceder?\n",
    "\n",
    "Uma forma de abordar este problema é converter as categorias em inteiros. Isto pode ser feito usando a classe ``OrdinalEncoder`` do Scikit-Learn 0.20 (o livro está desatualizado):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Separar apenas as variáveis categóricas (neste caso temos apenas uma).\r\n",
    "housing_cat = housing[['ocean_proximity']]\r\n",
    "\r\n",
    "print(type(housing_cat))\r\n",
    "print(housing_cat.head())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\r\n",
    "\r\n",
    "ordinal_encoder = OrdinalEncoder()\r\n",
    "\r\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\r\n",
    "housing_cat_encoded[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ordinal_encoder.categories_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para alguns problemas isso pode ser uma abordagem válida. Porém, na maioria dos casos isso não serve, pois gera uma hipótese implícita de que as categorias são ordenáveis, e que podemos somar e subtrair valores de categorias!\n",
    "\n",
    "A abordagem mais comum para a codificação de variáveis categóricas é convertê-las em uma representação vetorial, onde cada dimensão corresponde a uma categoria, e os valores em cada dimensão podem valer apenas zero ou um. Mais ainda, apenas uma dimensão valerá um, as demais valem zero. À esta representação chamamos 'one-hot encoding', por analogia com o mecanismo de sinalização em circuitos elétricos no qual apenas um fio está energizado por vez (one-hot).\n",
    "\n",
    "No nosso problema temos cinco categorias: ``'<1H OCEAN'``, ``'NEAR OCEAN'``, ``'INLAND'``, ``'NEAR BAY'`` e ``'ISLAND'``. Então vamos associar a categoria de um distrito a um vetor de dimensão 5, com valor um na dimensão correspondente à categoria do objeto, e zero no resto. \n",
    "\n",
    "Por exemplo, se o distrito tem categoria ``'NEAR OCEAN'``, transformamos esta informação no vetor ``(0, 1, 0, 0, 0)``. (Assumindo que as dimensões deste vetor correspondem às categorias na mesma ordem listada no parágrafo anterior.)\n",
    "\n",
    "No Scikit-Learn, a class OneHotEncoder recebe uma variável categórica qualquer (inteiros ou strings) e aplica a codificação one-hot."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cria o codificador.\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "encoder = OneHotEncoder(categories='auto')\r\n",
    "\r\n",
    "# Aprende a codificação e já aplica a mesma ao dataset fornecido.\r\n",
    "# Todo transformador no sklearn tem os métodos fit() para aprender\r\n",
    "# a transformação, e transform() para aplicá-la.\r\n",
    "# O método fit_transform() faz os dois atos em sequência.\r\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat)\r\n",
    "\r\n",
    "# O resultado da codificação é uma matriz esparsa em NumPy.\r\n",
    "print(housing_cat_1hot)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convertendo em matriz densa só para observar melhor:\r\n",
    "print(housing_cat_1hot.toarray()[:5])\r\n",
    "\r\n",
    "# Você poderia também ter usado sparse=False na criação do OneHotEncoder."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "encoder.categories_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Criando transformadores"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "``OneHotEncoder`` e ``SimpleImputer`` são exemplos de transformadores no Scikit-Learn: classes que podem ser treinadas para transformar dados. \n",
    "\n",
    "Você também pode criar novos tipos de transformadores! Vamos criar um transformador para adicionar as características de população-por-casa, comodos-por-casa e, opcionalmente, quartos-por-comodo.\n",
    "\n",
    "(Nota: no livro o autor discute o uso de flags para ligar/desligar comportamentos dos transformadores, para facilitar o teste de alternativas. São palavras sábias, e um padrão muito comum de desenvolvimento de software. Vai lá ler.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
    "\r\n",
    "\r\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\r\n",
    "    # column index\r\n",
    "    rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\r\n",
    "\r\n",
    "    def __init__(self, add_bedrooms_per_room=True):  # no *args or **kargs\r\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\r\n",
    "\r\n",
    "    def fit(self, X, y=None):\r\n",
    "        return self  # nothing else to do\r\n",
    "\r\n",
    "    def transform(self, X, y=None):\r\n",
    "        rooms_per_household = (X[:, CombinedAttributesAdder.rooms_ix] /\r\n",
    "                               X[:, CombinedAttributesAdder.household_ix])\r\n",
    "        population_per_household = (\r\n",
    "            X[:, CombinedAttributesAdder.population_ix] /\r\n",
    "            X[:, CombinedAttributesAdder.household_ix])\r\n",
    "\r\n",
    "        if self.add_bedrooms_per_room:\r\n",
    "            bedrooms_per_room = (X[:, CombinedAttributesAdder.bedrooms_ix] /\r\n",
    "                                 X[:, CombinedAttributesAdder.rooms_ix])\r\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\r\n",
    "                         bedrooms_per_room]\r\n",
    "        else:\r\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\r\n",
    "\r\n",
    "\r\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\r\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\r\n",
    "\r\n",
    "# Transformando em DataFrame, porque DataFrames são mais amigáveis.\r\n",
    "columns_housing_extra_attribs = list(housing.columns) + [\r\n",
    "    'rooms_per_household',\r\n",
    "    'population_per_household',\r\n",
    "]\r\n",
    "housing_extra_attribs = pd.DataFrame(\r\n",
    "    housing_extra_attribs,\r\n",
    "    columns=columns_housing_extra_attribs,\r\n",
    ")\r\n",
    "housing_extra_attribs.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Outro transformador importante é o ``StandardScaler``. Em muitos modelos de machine learning é importante que os dados não estejam em escalas numéricas muito diferentes, nem estejam em localizações médias muito diferentes da origem. Para tanto, é comum *normalizar* os dados. Lembram de Ciência dos Dados? Quando vocês normalizavam os dados para testá-los usando a curva normal padrão? Bem, é a mesma coisa aqui: remover a média e dividir pelo desvio padrão. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pipelines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uma pipeline é uma sequência de operações. Scikit-Learn tem uma classe dedicada à construção de pipelines, onde vários transformers podem ser encadeados em uma pipeline para se comportar como um único transformer.\n",
    "\n",
    "Vamos construir uma pipeline para encadear as várias transformações que aplicamos aos nossos dados numéricos:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "meu_imputer = SimpleImputer(strategy='median')\r\n",
    "meu_adder = CombinedAttributesAdder()\r\n",
    "meu_scaler = StandardScaler()\r\n",
    "\r\n",
    "num_pipeline = Pipeline([\r\n",
    "    ('imputer', meu_imputer),\r\n",
    "    ('attribs_adder', meu_adder),\r\n",
    "    ('std_scaler', meu_scaler),\r\n",
    "])\r\n",
    "\r\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\r\n",
    "housing_num_tr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note que os valores já não são os mesmos de ``housing_num``, por conta do ``StandardScaler``.\n",
    "\n",
    "A variável categórica também merece uma 'pipeline' de um estágio só agora - bear with me for now..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "meu_one_hot_encoder = OneHotEncoder(sparse=False)\r\n",
    "\r\n",
    "cat_pipeline = Pipeline([\r\n",
    "    ('cat_encoder', meu_one_hot_encoder),\r\n",
    "])\r\n",
    "\r\n",
    "housing_cat_tr = cat_pipeline.fit_transform(housing_cat)\r\n",
    "housing_cat_tr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ao invés de aplicar nossas pipelines à variáveis ``housing_cat`` e ``housing_num``, seria interessante aplicá-las simplesmente à  ``housing``. Para isso precisamos de um transformer que:\n",
    "\n",
    "- Faça a seleção de colunas de ``housing`` para separar as variáveis contínuas das categóricas, \n",
    "- Aplique as respectivas pipelines transformadoras e\n",
    "- Junte os resultados.\n",
    "\n",
    "Essas atividades podem ser construidas com a classe ``ColumnTransformer`` do Scikit-Learn:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.compose import ColumnTransformer\r\n",
    "\r\n",
    "num_attribs = list(housing_num)\r\n",
    "cat_attribs = ['ocean_proximity']\r\n",
    "\r\n",
    "full_pipeline = ColumnTransformer([\r\n",
    "    ('num', num_pipeline, num_attribs),\r\n",
    "    ('cat', cat_pipeline, cat_attribs),\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Esta pipeline final pode ser aplicada para transformar os dados de treinamento originais em dados de treinamento processados, prontos para treinar o modelo preditivo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing_prepared = full_pipeline.fit_transform(housing)\r\n",
    "housing_prepared[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Em resumo, construimos uma pipeline que:\n",
    "\n",
    "- Codifica adequadamente as variáveis categóricas, usando one-hot encoding.\n",
    "- Preenche os buracos do dataset com valores medianos, usando o Imputer.\n",
    "- Adiciona novas features.\n",
    "- Normaliza os dados, para evitar problemas com alguns modelos de machine learning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Atividade:**\n",
    "\n",
    "Crie um notebook e 'passe a limpo' o material desenvolvido até agora: copie os códigos essenciais para\n",
    "\n",
    "- Ler os dados\n",
    "\n",
    "- Separar de modo estratificado os dados em conjunto de treinamento e teste\n",
    "\n",
    "- Criar a pipeline de preparação dos dados. A mesma pipeline será usada mais tarde no conjunto de testes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construindo modelos preditivos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente é chegada a hora de construir modelos preditivos! O modelo mais simples de regressão é a regressão linear, na qual desejamos estimar um valor dependente como uma combinação linear dos valores independentes (mais um termo constante).\n",
    "\n",
    "Treinar um modelo no Scikit-Learn é simples: basta criar um regressor, e chamar o método ``fit()`` deste regressor para ajustar os parâmetros internos do modelo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression\r\n",
    "\r\n",
    "lin_reg = LinearRegression()\r\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos selecionar alguns pontos de dados para demonstrar o funcionamento do nosso regressor:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Seleciona 5 pontos do conjunto de treinamento.\r\n",
    "some_data = housing.iloc[:5]\r\n",
    "some_labels = housing_labels.iloc[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "some_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "some_labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prepara os dados - não se esqueça deste passo.\r\n",
    "some_data_prepared = full_pipeline.transform(some_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "some_data_prepared"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Para obter as previsões, basta chamar o método predict()\r\n",
    "predicted_labels = lin_reg.predict(some_data_prepared)\r\n",
    "print('Predição: {}'.format(predicted_labels.round(decimals=2)))\r\n",
    "\r\n",
    "# Compare com os valores originais:\r\n",
    "print('Original: {}'.format(some_labels.values.round(decimals=2)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parece que está funcionando, aproximadamente!\n",
    "\n",
    "Pergunta: por que é muito otimista dizer que as coisas estão funcionando bem agora?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos medir o erro de predição no conjunto de treinamento:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\r\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\r\n",
    "lin_rmse = np.sqrt(lin_mse)\r\n",
    "print('Regressão linear: RMSE = {:.2f}'.format(lin_rmse))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "residuo = housing_labels - housing_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.hist(residuo, bins=50);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.Series(residuo).describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "E se a gente trocasse de regressor? Vamos tentar aplicar uma árvore de decisão. Primeiro, treinar o modelo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\r\n",
    "\r\n",
    "tree_reg = DecisionTreeRegressor(random_state=RANDOM_SEED)\r\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora, medir o erro do modelo:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predicted_labels = tree_reg.predict(some_data_prepared)\r\n",
    "print('Predição: {}'.format(predicted_labels))\r\n",
    "print('Original: {}'.format(some_labels.values))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Opa, meio suspeito isso! Vamos ver o erro total:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\r\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\r\n",
    "tree_rmse = np.sqrt(tree_mse)\r\n",
    "print('Regressão linear: RMSE = {:.2f}'.format(tree_rmse))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bem, parece que achamos o regressor perfeito! Na verdade, já vimos esse fenômeno antes...\n",
    "\n",
    "Pergunta: o que aconteceu aqui?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Melhorando nossa avaliação usando validação cruzada"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos lembrar onde estamos neste processo: nossa tarefa no momento é:\n",
    "\n",
    "- Escolher uma família de modelos de machine learning que seja adequada ao nosso problema\n",
    "\n",
    "- Escolher os melhores parâmetros e hiperparâmetros para nosso modelo\n",
    "\n",
    "Para escolher um bom modelo, temos que tentar estimar o erro de teste do modelo. Por que não simplesmente usar o conjunto de testes aqui? Porque *só vamos tocar no conjunto de testes DEPOIS QUE TODAS AS DECISÕES JÁ TENHAM SIDO TOMADAS!*\n",
    "\n",
    "Então tudo o que temos é o conjunto de treinamento. E gostaríamos de treinar nosso modelo em um certo conjunto de dados, e testá-lo em outro conjunto de dados. Como resolver esse dilema?\n",
    "\n",
    "Simples: vamos novamente dividir o conjunto de treinamento em dois! Usamos um pedaço para treinar modelos, e outro para testar desempenho. À esta validação de modelos usando apenas o conjunto de treinamento chamamos ***validação cruzada***.\n",
    "\n",
    "Por simplicidade vamos usar divisão não-estratificada, e vamos dividir diretamente o dataset pós-pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "    housing_prepared,\r\n",
    "    housing_labels,\r\n",
    "    test_size=0.2,\r\n",
    "    random_state=RANDOM_SEED,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lin_reg.fit(X_train, y_train)\r\n",
    "\r\n",
    "y_pred = lin_reg.predict(X_test)\r\n",
    "lin_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\r\n",
    "print('Regressão linear: RMSE = {:.2f}'.format(lin_rmse))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tree_reg.fit(X_train, y_train)\r\n",
    "\r\n",
    "y_pred = tree_reg.predict(X_test)\r\n",
    "tree_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\r\n",
    "print('Regressão árvore de decisão: RMSE = {:.2f}'.format(tree_rmse))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora faz mais sentido! Tanto o regressor linear quanto o regressor de árvore de decisão tem desempenho similar! Vamos tentar mais um regressor, desta vez um regressor poderoso chamado Random Forest:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "\r\n",
    "forest_reg = RandomForestRegressor(n_estimators=10, random_state=RANDOM_SEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "forest_reg.fit(X_train, y_train)\r\n",
    "\r\n",
    "y_pred = forest_reg.predict(X_test)\r\n",
    "forest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\r\n",
    "print('Regressão random forest: RMSE = {:.2f}'.format(forest_rmse))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parece que o random forest é melhor que os outros!\n",
    "\n",
    "Mas talvez todos esses resultados sejam pura sorte: como saber? Podemos repetir esses experimentos com partições diferentes e ver se o resultado se mantém. O scikit-learn já tem ferramentas para ajudar nessa tarefa:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import cross_val_score\r\n",
    "\r\n",
    "lin_scores = cross_val_score(\r\n",
    "    lin_reg,  # modelo\r\n",
    "    housing_prepared,  # X_train\r\n",
    "    housing_labels,  # y_train\r\n",
    "    scoring='neg_mean_squared_error',  # metrica de interesse\r\n",
    "    cv=10,  # Quantas partições eu quero\r\n",
    "    n_jobs=-1,  # Use todos os cores da maquina!\r\n",
    ")\r\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\r\n",
    "\r\n",
    "\r\n",
    "def display_scores(scores):\r\n",
    "    print('Scores:', scores.round(decimals=2))\r\n",
    "    print('Mean:', scores.mean())\r\n",
    "    print('Standard deviation:', scores.std())\r\n",
    "\r\n",
    "\r\n",
    "display_scores(lin_rmse_scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O código acima executa ***n-fold cross validation*** (neste caso, $n=10$). A função ``cross_val_score`` divide o conjunto de treinamento em $n$ partes e executa o procedimento de testes (treinar modelo, prever, medir erro) $n$ vezes - uma para cada partição. A cada ensaio a partição da vez é separada como conjunto de teste, e as demais compõe o conjunto de treinamento.\n",
    "\n",
    "Uma vantagem desta abordagem é que agora podemos ver a faixa de desempenhos do modelo.\n",
    "\n",
    "Vamos repetir esta atividade para os outros regressores:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tree_scores = cross_val_score(\r\n",
    "    tree_reg,\r\n",
    "    housing_prepared,\r\n",
    "    housing_labels,\r\n",
    "    scoring='neg_mean_squared_error',\r\n",
    "    cv=10,\r\n",
    "    n_jobs=-1,\r\n",
    ")\r\n",
    "tree_rmse_scores = np.sqrt(-tree_scores)\r\n",
    "display_scores(tree_rmse_scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "forest_scores = cross_val_score(\r\n",
    "    forest_reg,\r\n",
    "    housing_prepared,\r\n",
    "    housing_labels,\r\n",
    "    scoring='neg_mean_squared_error',\r\n",
    "    cv=10,\r\n",
    "    n_jobs=-1,\r\n",
    ")\r\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\r\n",
    "display_scores(forest_rmse_scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.DataFrame({\r\n",
    "    'Linear': lin_rmse_scores,\r\n",
    "    'Decision Tree': tree_rmse_scores,\r\n",
    "    'Random Forest': forest_rmse_scores,\r\n",
    "}).plot.box(\r\n",
    "    xlabel='Regressor',\r\n",
    "    ylabel=r'RMSE $[\\mathtt{USD}]$',\r\n",
    "    figsize=(6, 4),\r\n",
    ");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora sim podemos dizer, com segurança, que o regressor random forest é melhor que os outros!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pergunta: Podemos mesmo dizer isso? Como cientista de dados, como você responderia essa questão com mais segurança?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R:**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ajuste de hiperparâmetros"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mas afinal, o que é um modelo de regressão? É uma função que transforma os dados de entrada em um valor de saída, e que também pode depender de alguns **parâmetros**:\n",
    "\n",
    "$$y = h(x; \\theta)$$\n",
    "\n",
    "Treinar o modelo é ajustar os parâmetros do modelo para maximizar o desempenho preditivo deste. Para tanto devemos usar um algoritmo de treinamento. Cada classe de modelos demanda seu próprio algoritmo de treinamento, vamos estudar isso em detalhes mais tarde.\n",
    "\n",
    "$$\\theta_{opt} = \\text{argmin}_{\\theta} \\{ \\text{RMSE}\\left(X_{\\text{train}}, y_{\\text{train}}, h_{\\theta} \\right) \\}$$\n",
    "\n",
    "Plot twist: os algoritmos de treinamento em si *também* tem seus parâmetros! Ademais, os modelos tem parâmetros que especificam sub-classes de modelos, e diferem dos parâmetros voltados ao 'ajuste fino'. A esses meta-parâmetros chamamos **hiperparâmetros**.\n",
    "\n",
    "Os parâmetros regulares são ajustados pelo método ``fit()`` dos regressores. Como ajustar os hiperparâmetros? A abordagem mais simples é testar vários valores e ver o que funciona! Existem abordagens mais sofisticadas, que discutiremos depois, mas por hoje vamos testar uma dessas abordagens 'força-bruta' chamada *grid search*.\n",
    "\n",
    "Funciona assim: escolha alguns valores possíveis de hiperparâmetros, e teste todas as combinações. Vamos aplicar isso ao regressor random forest. Não se preocupe com o significado destes hiperparâmetros por enquanto, vamos estudar isso em detalhes depois.\n",
    "\n",
    "Em scikit-learn, temos uma classe ``GridSearchCV`` para fazer isso. *AVISO*: vai demorar!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\r\n",
    "\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "param_grid = [\r\n",
    "    # try 6 (2×3) combinations of hyperparameters.\r\n",
    "    {\r\n",
    "        'n_estimators': [10, 30],\r\n",
    "        'max_features': [4, 6, 8],\r\n",
    "    },\r\n",
    "    # then try 4 (1x2×2) combinations with bootstrap set as False.\r\n",
    "    {\r\n",
    "        'bootstrap': [False],\r\n",
    "        'n_estimators': [3, 10],\r\n",
    "        'max_features': [3, 4],\r\n",
    "    },\r\n",
    "]\r\n",
    "\r\n",
    "forest_reg = RandomForestRegressor(random_state=RANDOM_SEED)\r\n",
    "\r\n",
    "# train across 5 folds, that's a total of (6+4)*5=50 rounds of training.\r\n",
    "grid_search = GridSearchCV(\r\n",
    "    forest_reg,  # Modelo\r\n",
    "    param_grid,  # Grid\r\n",
    "    cv=5,  # Partições de C.V.\r\n",
    "    scoring='neg_mean_squared_error',\r\n",
    "    return_train_score=True,\r\n",
    "    n_jobs=-1,\r\n",
    ")\r\n",
    "\r\n",
    "t1 = time.perf_counter()\r\n",
    "grid_search.fit(housing_prepared, housing_labels)\r\n",
    "t2 = time.perf_counter()\r\n",
    "\r\n",
    "print(f'Tempo gasto: {t2 - t1:.2f} s')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O procedimento procurou a melhor combinação de hiperparâmetros para o nosso regressor:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dica: se os melhores hiperparâmetros forem o limite superior da faixa de valores testados, talvez valha a pena aumentar a faixa de pesquisa!\n",
    "\n",
    "Pergunta: neste nosso caso valeria a pena refinar a faixa de pesquisa?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**R**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O ``GridSearch`` já retorna o melhor modelo treinado:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search.best_estimator_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para ver como o ``GridSearch`` achou a melhor solução, podemos ver o histórico de desempenhos testados!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cvres = grid_search.cv_results_\r\n",
    "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\r\n",
    "    print(np.sqrt(-mean_score), params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importância das características"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para alguns modelos de machine learning podemos obter a importância relativa das características no processo de predição. Esta informação é importante para entender melhor nosso problema. De fato, um dos usos bastante importantes do machine learning é exatamente isso: usar o machine learning para entender melhor o problema em si!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\r\n",
    "feature_importances"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para saber quem-é-quem:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "extra_attribs = [\r\n",
    "    'rooms_per_hhold',\r\n",
    "    'pop_per_hhold',\r\n",
    "    'bedrooms_per_room',\r\n",
    "]\r\n",
    "cat_one_hot_attribs = list(\r\n",
    "    cat_pipeline.named_steps['cat_encoder'].categories_[0])\r\n",
    "\r\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\r\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parece que nossa intuição de adicionar as features extras foi acertada: estas novas features são mais importantes do que os dados brutos originais!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Finalmente: medir o desempenho final!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O que fizemos até agora, após a preparação de dados?\n",
    "\n",
    "- Usamos validação cruzada para achar a melhor família de regressores para nosso modelo. Note que nesta etapa não ajustamos hiperparâmetros, apenas confiamos nos valores default.\n",
    "\n",
    "- Usamos novamente validação cruzada para achar os melhores hiperparâmetros, com busca no espaço de hiperparâmetros.\n",
    "\n",
    "Agora temos o nosso melhor modelo, treinado na forja da validação cruzada! Chegou finalmente a hora de medir o desempenho do regressor no conjunto de testes!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_model = grid_search.best_estimator_\r\n",
    "\r\n",
    "X_test = strat_test_set.drop('median_house_value', axis=1)\r\n",
    "y_test = strat_test_set['median_house_value'].copy()\r\n",
    "\r\n",
    "X_test_prepared = full_pipeline.transform(X_test)\r\n",
    "final_predictions = final_model.predict(X_test_prepared)\r\n",
    "\r\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\r\n",
    "final_rmse = np.sqrt(final_mse)\r\n",
    "\r\n",
    "print(f'RMSE = {final_rmse}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Atividade:** Vá para o seu outro notebook e continue 'passando a limpo' esta atividade. Transporte os processos essenciais de treinamento para o novo notebook, e a avaliação final de desempenho."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusão"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Você acaba de terminar um projeto completo de regressão, onde fizemos o seguinte:\n",
    "\n",
    "- Carregamos os dados\n",
    "    - Se o dataset fosse muito grande, teríamos selecionado um subset pequeno para exploração.\n",
    "- Separamos os dados em conjunto de treinamento e teste.\n",
    "- Visualizar e explorar os dados para entendê-los melhor\n",
    "- Preparar os dados para machine learning\n",
    "- Escolher uma boa família de modelos\n",
    "- Treinar os modelos, fazer ajuste fino dos hiperparâmetros.\n",
    "- Testar desempenho no conjunto de testes\n",
    "\n",
    "Você aprendeu várias coisas valiosas sobre o dataset ('A renda mediana é o melhor preditor de valores de imóveis'), estimou o desempenho do seu modelo, e está pronto para implementar seu modelo em um sistema de produção! Agora é hora de montar uma boa apresentação sobre os seus resultados para o cliente (ou o chefe).\n",
    "\n",
    "Com isso concluimos a etapa de definição do modelo. O que mais temos que fazer?\n",
    "\n",
    "- Agora JUNTE OS DADOS DE TREINAMENTO E DE TESTE (ou seja, o *dataset* completo) E TREINE O SEU MODELO **FINAL**. Essa história de separar em treinamento e teste é para estudar o desempenho do modelo apenas! Para fazer o DEPLOY em produção, retreine o modelo com os dados completos!\n",
    "- Faça o deploy do modelo. Muitas vezes isso envolve a construção de um *microsserviço*. Você deverá trabalhar junto com o engenheiro de *dev-ops* para garantir que o deploy é robusto e escalável.\n",
    "- **Monitore** o desempenho do modelo em produção. O modelo reflete o conhecimento embutido nos dados, e não tem bola-de-cristal para prever mudanças de conteudo com o passar do tempo (*concept drift*). Em algum momento esse modelo estará desatualizado e você deverá retreiná-lo **com novos dados**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}