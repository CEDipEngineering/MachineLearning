{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base de dados que utilizaremos é conhecida como MNIST (\"Modified National Institute of Standards and Technology\"), e deriva de uma base maior que foi construida pela NIST nos Estados Unidos (o equivalente da nossa ABNT). Esta base de dados é considerada o verdadeiro \"Hello, world!\" de métodos de classificação. Em http://yann.lecun.com/exdb/mnist/ temos uma descrição mais detalhada desta base de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade**: \n",
    "    \n",
    "Leia a página de descrição do MNIST supracitada, e responda:\n",
    "\n",
    "- Quantas imagens de treinamento e quantas imagens de teste existem na MNIST?\n",
    "\n",
    "- Qual o tamanho de cada imagem no MNIST?\n",
    "\n",
    "- Os criadores da MNIST tiveram um cuidado especial ao construir os conjuntos de treinamento e teste, em relação às pessoas que escreveram os dígitos. Que cuidado foi esse, e por que foi adotado?\n",
    "\n",
    "- As imagens foram construidas escaneando digitos manuscritos, que foram escritos por dois grupos de pessoas: alunos de colegial (SD-1) e funcionários da NIST (SD-3). Originalmente a NIST designou SD-3 como o conjunto de teste, e SD-1 como o conjunto de treinamento. Os criadores da MNIST criticaram essa decisão e resolveram misturar os conjuntos. Por que? Como esta situação difere daquela da pergunta anterior?\n",
    "\n",
    "- A página lista vários artigos que exploraram métodos de classificação no MNIST, com seus respectivos desempenhos. Qual o método com o pior desempenho (e qual foi esse desempenho)? Qual o método com o melhor desempenho, e de quanto foi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "- Quantas imagens de treinamento e quantas imagens de teste existem na MNIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com o site:\n",
    "\n",
    "    The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "    \n",
    "Portanto, 60000 imagens de treinamento e 10000 imagens de teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Qual o tamanho de cada imagem no MNIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com o site:\n",
    "\n",
    "    The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.\n",
    "    \n",
    "Portanto as imagens são de tabanho $28 \\times 28$ pixels. Interessante notar que o centro de massa de cada imagem foi posicionado no centro deste campo de $28 \\times 28$ pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Os criadores da MNIST tiveram um cuidado especial ao construir os conjuntos de treinamento e teste, em relação às pessoas que escreveram os dígitos. Que cuidado foi esse, e por que foi adotado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomou-se o cuidado de separar as pessoas: se os digitos escritos por uma pessoa estão no conjunto de imagens de treinamento, então dígitos escritos por essa pessoa não podem aparecer no conjunto de teste, e vice-versa. A lógica que leva a essa medida é considerar que um classificador de dígitos deve ser apto a identificar dígitos manuscritos de uma pessoa que nunca foi vista na fase de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As imagens foram construidas escaneando digitos manuscritos, que foram escritos por dois grupos de pessoas: alunos de colegial (SD-1) e funcionários da NIST (SD-3). Originalmente a NIST designou SD-3 como o conjunto de teste, e SD-1 como o conjunto de treinamento. Os criadores da MNIST criticaram essa decisão e resolveram misturar os conjuntos. Por que? Como esta situação difere daquela da pergunta anterior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os criadores da base MNIST resolveram misturar as pessoas dos grupos SD-1 e SD-3. Isto é necessário para garantir que o classificador é efetivo para dígitos escritos por pessoas de ambos os grupos. Caso essa mistura não fosse realizada correríamos o risco de ter um classificador que só é efetivo para reconhecer a caligrafia de alunos do colegial, o que é um erro de projeto. \n",
    "\n",
    "A situação atual difere da anterior: a medida tomada na questão anterior dizia respeito a verificar o comportamento do classificador para uma nova pessoa, mas de uma categoria de pessoas já conhecida. A medida atual diz respeito a testar o desempenho do classificador para grupos conhecidos de pessoas, sem viés sistêmico do *dataset* de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A página lista vários artigos que exploraram métodos de classificação no MNIST, com seus respectivos desempenhos. Qual o método com o pior desempenho (e qual foi esse desempenho)? Qual o método com o melhor desempenho, e de quanto foi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método de melhor desempenho dentre as técnicas elencadas é o método do *technical report* \"Multi-column Deep Neural Networks for Image Classification\" de Dan Cireşan, Ueli Meier e Juergen Schmidhuber.\n",
    "\n",
    "\n",
    "A técnica aplicada é descrita como \"committee of 35 conv. net, 1-20-P-40-P-150-10 [elastic distortions]\", usando pré-processamento \"width normalization\" resultando em uma taxa de erro de classificação no conjunto de treinamento de $0,23 \\%$ (ou seja, uma acurácia de $99,77 \\%$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O scikit-learn já tem ferramentas para baixar e disponibilizar alguns dos datasets mais comuns da comunidade de machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\jpgia\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " array([5, 0, 4, ..., 4, 5, 6], dtype=int8))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "\n",
    "# A função fetch_openml() returns targets as strings, precisamos converter para\n",
    "# valores numéricos.\n",
    "mnist.target = mnist.target.astype(np.int8)\n",
    "\n",
    "mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No campo `data` temos as várias imagens de dígitos manuscritos. Cada item é uma lista de $28^2 = 784$ valores.\n",
    "\n",
    "No campo `target` temos o rótulo de cada uma dessas imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5daaf603e17c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "X, y = mnist['data'].to_numpy(), mnist['target'].to_numpy()\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver um desses dígitos manuscritos para checar se a leitura de dados funcionou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit = X[0]\n",
    "some_digit_label = y[0]\n",
    "\n",
    "print(f'label: {some_digit_label}')\n",
    "\n",
    "some_digit_image = some_digit.reshape(28, 28)  # Por que? Explique!\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(\n",
    "    some_digit_image,\n",
    "    cmap=matplotlib.cm.binary,\n",
    "    interpolation='nearest',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que deu certo: pela imagem trata-se de um dígito $5$ manuscrito, e de fato o rótulo confirma essa observação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando treinamento e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme visto na descrição do dataset MNIST, a separação entre conjunto de treinamento e teste já está feita. Neste caso, não devemos fazer a separação dos dados conforme visto na aula passada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(mnist['target'][:60000], return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f'{u}: {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os primeiros 60000 exemplos são o conjunto de treinamento, e estão organizados por dígito. Os últimos 10000 exemplos são o conjunto de teste, e também estão organizados por dígito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver mais alguns digitos desta base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    \"\"\"Plota uma lista de imagens do MNIST em um formato de matriz de imagens.\n",
    "    \n",
    "    Args:\n",
    "        instances: Lista de linhas da matriz de amostras do MNIST.\n",
    "        images_per_row: Número de imagens por linha.\n",
    "        options: Opções passadas ao comando plt.imshow() para desenho.\n",
    "    \"\"\"\n",
    "    # Tamanho das imagens no MNIST: 28 x 28.\n",
    "    size = 28\n",
    "\n",
    "    # Monta uma lista de imagens usando list comprehension e np.reshape().\n",
    "    # Cada item da lista resultante é uma imagem 28 x 28.\n",
    "    images = [instance.reshape(size, size) for instance in instances]\n",
    "\n",
    "    # Caso o número de imagens por linha seja muito grande,\n",
    "    # limite no número de imagens disponível.\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "\n",
    "    # Dado o número de imagens por linha, calcule quantas linhas\n",
    "    # são necessárias.\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "\n",
    "    # Truque: cria uma imagem em branco de tamanho suficiente para preencher\n",
    "    # o espaço em branco no final da ultima linha, e coloca essa imagem\n",
    "    # em branco na lista de imagens.\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "\n",
    "    # Cria uma imagem unificada por linha.\n",
    "    row_images = []\n",
    "    for row in range(n_rows):\n",
    "        # Junta imagens da linha em uma imagem unificada, e coloca na\n",
    "        # lista de imagens de linha.\n",
    "        rimages = images[row * images_per_row:(row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "\n",
    "    # Junta todas as imagens de linha em uma unica imagem final.\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "\n",
    "    # Mostra a imagem final.\n",
    "    plt.imshow(image, cmap=matplotlib.cm.binary, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "example_labels = y[:100]\n",
    "example_images = X[:100]\n",
    "\n",
    "print(example_labels)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação binária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar com um problema mais simples: classificar os dígitos da base em 'cincos' e 'não-cincos'. Este é um problema de classificação binária. Por mera convenção, chamaremos de 'amostras positivas', ou simplemente 'positivos' os digitos $5$ e de 'negativos' os demais dígitos.\n",
    "\n",
    "Vamos adaptar os conjuntos de treinamento e teste ao nosso cenário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando se funcionou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "for original, binarized in zip(y_train[:n], y_train_5[:n]):\n",
    "    print(f'{original} -> {binarized}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora treinar um classificador sobre todo o conjunto de treinamento, como fizemos na aula sobre regressão. Vamos usar um classificador chamado de *Stochastic Gradient Descent*, que é uma generalização de alguns tipos diferentes de classificadores mais tradicionais. O scikit-learn tem uma classe que implementa este classificador: `SGDClassifier`. Com os parâmetros default desta classe, o classificador SGD é equivalente a um classificador do tipo \"máquina de vetores de suporte linear\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Existe aleatoriedade dentro do SGDClassifier, por isso o argumento\n",
    "# random_state=RANDOM_SEED.\n",
    "sgd_clf = SGDClassifier(\n",
    "    max_iter=500,\n",
    "    tol=1e-3,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar o classificador naquele dígito $5$ que a gente tinha visualizado no começo do notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe que a amostra \"some_digit\" é apenas uma lista de valores de pixel.\n",
    "# O comando predict requer uma matriz ou uma lista de listas, onde cada linha é\n",
    "# uma amostra a ser classificada (mesmo que seja uma amostra só!). Portanto,\n",
    "# temos que colocar \"some_digit\" numa lista.\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfeito, ele acertou! Mas isso foi apenas um exemplo, vamos agora estudar o desempenho do classificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de desempenho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando validação cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como na aula anterior, podemos usar a estratégia da validação cruzada para tentar inferir o desempenho do nosso classificador no mundo real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "t1 = time.process_time()\n",
    "res = cross_val_score(\n",
    "    sgd_clf,\n",
    "    X_train,\n",
    "    y_train_5,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "t2 = time.process_time()\n",
    "\n",
    "print('Elapsed time: {}'.format(t2 - t1))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida usada é o *accuracy* (acurácia), que é a porcentagem de acertos de previsão. Obtivemos 96%! Parece excelente, mas será mesmo? Compare com o \"classificador\" a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Nada a ser feito no treinamento.\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Recebe len(X) amostras, chuta \"False\" como resposta para todas!\n",
    "        return [False] * len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(\n",
    "    never_5_clf,\n",
    "    X_train,\n",
    "    y_train_5,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pergunta:** Parece que atingir 90% não é nada difícil neste problema... na verdade, é o esperado! Explique porque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "Em um dataset de dígitos ao acaso, esperamos que $90\\%$ destes não sejam o dígito $5$. Se nosso classificador simplesmente \"chuta\" que um dígito qualquer não é o dígito $5$, vai acertar em $90 \\%$ dos casos. \n",
    "\n",
    "Com isso podemos ver que em qualquer problema de classificação é importante conhecer a distribuição das classes, pois sempre podemos construir um classificador trivial que responde à qualquer consulta com a classe mais frequente do dataset. Tal classificador trivial terá uma acurácia média equivalente à frequência relativa da classe mais provável. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma muito interessante de se avaliar o desempenho de um classificador é obter a matriz de confusão (*confusion matrix*) do classificador. Nesta matriz cada linha representa a categoria *verdadeira* de um objeto, e cada coluna representa a categoria *predita* de um objeto. Uma posição $(r,c)$ da matriz de confusão representa, portanto, o número de objetos que pertencem verdadeiramente à categoria $r$, mas que foram classificados como pertencentes à categoria $c$ por nosso classificador. \n",
    "\n",
    "<center>\n",
    "<img src=\"confusao.png\" alt=\"Matriz de confusão\">\n",
    "</center>\n",
    "\n",
    "As células da diagonal, em azul, mostram as posições onde a classe verdadeira e a classe predita coincidem, esses são os acertos. As células fora da diagonal, em vermelho, são os erros.\n",
    "\n",
    "Podemos calcular a matriz de confusão resultante do treinamento sobre o conjunto (de treinamento) completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "y_train_pred = sgd_clf.predict(X_train)\n",
    "mat = confusion_matrix(y_train_5, y_train_pred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porém o desempenho exibido por este processo é muito otimista, e não representa uma estimativa realista dos erros deste classificador no mundo real.\n",
    "\n",
    "**Pergunta:** Em caso de overfitting total, como ficaria a matriz de confusão?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:** diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma idéia melhor é aplicar o conceito de validação cruzada para realizar a predição de cada amostra. Funciona assim:\n",
    "\n",
    "- Particionamos os dados em N partições.\n",
    "\n",
    "- Para cada partição:\n",
    "\n",
    "    - Treinamos o classificador sobre os dados das outras partições\n",
    "    \n",
    "    - Usamos o classificador para prever as classes das amostras desta partição\n",
    "\n",
    "Por exemplo: suponha que temos 3 partições. As categorias preditas dos objetos da primeira partição são obtidas da seguinte forma:\n",
    "\n",
    "- Treinamos o classificador usando os dados das partições 2 e 3\n",
    "\n",
    "- Aplicamos o classificador para os objetos da partição 1. Guardamos estes resultados\n",
    "\n",
    "Fazemos o mesmo para os objetos das partições 2 e 3. Desta forma, cada objeto foi predito de modo \"honesto\", ou seja, usando um classificador que não continha o próprio objeto como dado de treinamento!\n",
    "\n",
    "O scikit-learn já tem uma função para fazer exatamente isso: `cross_val_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos observar uma matriz de confusão mais realista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_train_5, y_train_pred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não mudou muito, mas tudo bem: esses números são mais confiáveis. Significa que nosso modelo não é muito inclinado a ter overfitting nesse problema em particular!\n",
    "\n",
    "Nesta matriz de confusão a primeira linha indica dígitos \"não-cinco\", e a segunda linha indica os dígitos \"cinco\". Em problemas de classificação binária usamos a terminologia \"negativos\" (os \"não-cinco\") e \"positivos\" (os \"cinco\"), e dizemos também que nosso problema é \"detectar\" os dígitos \"cinco\".\n",
    "\n",
    "- **TP**: Os valores **verdadeiramente positivos** e que foram **preditos como positivos** são os **true positives** (verdadeiros positivos).\n",
    "\n",
    "- **FN**: Os valores **verdadeiramente positivos** e que foram **preditos como negativos** são os **false negatives** (falsos negativos, pois foram errôneamente classificados como negativos).\n",
    "\n",
    "- **TN**: Os valores **verdadeiramente negativos** e que foram **preditos como negativos** são os **true negatives** (verdadeiros negativos).\n",
    "\n",
    "- **FP**: os valores **verdadeiramente negativos** e que foram **preditos como positivos** são os **false positives** (falsos positivos, pois foram errôneamente classificados como positivos).\n",
    "\n",
    "![Precision-recall](precision_recall.png \"Precision and recall\")\n",
    "\n",
    "**Perguntas:**\n",
    "\n",
    "- Dê um exemplo real de falso positivo.\n",
    "\n",
    "- Dê um exemplo real de falso negativo.\n",
    "\n",
    "- Nesta matriz exemplo, qual é a acurácia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "- Dê um exemplo real de falso positivo.\n",
    "\n",
    "- Dê um exemplo real de falso negativo.\n",
    "\n",
    "\n",
    "Considere a seguinte situação: você quer contratar um novo engenheiro, e para tanto vai entrevistar um conjunto de candidatos. Você quer achar um engenheiro que conheça bem *machine learning*. No dia das entrevistas você teve um problema e não pode ir entrevistar os candidatos: no seu lugar foi um colega da equipe que não conhece tanto assim sobre *machine learning*. E agora?\n",
    "\n",
    "Seu colega é como um classificador ruim. Ele pode acabar achando que um candidato é excelente (positivo) quando na verdade ele é ruim (portanto, falso positivo) - por exemplo, as perguntas feitas não tinham muito a ver com *machine learning*, e o candidato acabou se saindo bem. Por outro lado, o entrevistador pode acabar rejeitando (negativo) um candidato muito bom (portanto, falso negativo) porque as perguntas acabaram não sendo efetivas para detectar a competência do candidato no assunto central.\n",
    "\n",
    "É por isso que os processos seletivos concorridos geralmente são organizados da seguinte forma:\n",
    "\n",
    "- Fase inicial: seleção de currículos. Faz-se uma seleção abrangente, com um critério relaxado de aceitação (alto *recall*, baixo *precision* - ver abaixo) para capturar a maioria dos candidatos interessantes, mas deixando algum excesso de candidatos ruins. Nesta fase não precisamos de grande acurácia, e esta fase tende a ser conduzida pelo departamento de recursos humanos.\n",
    "\n",
    "- Entrevistas preliminares. Geralmente temos alguns rounds de entrevistas telefônicas rápidas, feita por engenheiros do time. Nestas entrevistas temos um aumento de *precision* mantendo um bom *recall*, mas o custo do processo é mais alto - precisa envolver tempo de engenheiros.\n",
    "\n",
    "- Entrevistas on site. Nesta fase os candidatos são convidados para entrevistas face-a-face com os engenheiros mais seniores. Trata-se de uma fase de alto custo: além do tempo requerido de membros seniores da equipe, muitas vezes os custos de deslocamento e estadia dos candidatos é pago pela própria empresa. A razão pela qual esta etapa é tão custosa é que o preço de um erro aqui é alto: \n",
    "\n",
    "    - Falso positivo: contrata-se um engenheiro que se mostra ineficiente no trabalho cotidiano. Neste caso existem custos associados ao treinamento do funcionário, custos de oportunidade por não ter contratado alguém melhor e pelo tempo e recursos gastos com a ineficiência deste funcionário, e custos de demissão.\n",
    "    \n",
    "    - Falso negativo: custo de oportunidade de não ter contratado alguém bom, e também o fato de que este candidato estará disponível para contratação pelos competidores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nesta matriz exemplo, qual é a acurácia?\n",
    "\n",
    "Temos um total de $8$ acertos ($5$ true negatives e $3$ true positives) de um total de $11$ amostras, logo a acurácia é $8 / 11 \\approx 72,7 \\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision e recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida de acurácia não permite distinguir entre os tipos de erro. Duas medidas mais comuns que são empregadas em machine learning são a **precision** (precisão) e **recall** (revocação), definidas como:\n",
    "\n",
    "- Precision: Dentre os elementos classificados como positivos, quantos realmente são positivos?\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "- Recall: Dentre os elementos verdadeiramente positivos, quantos foram detectados como positivos?\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**Perguntas:** \n",
    "\n",
    "- É sempre possível construir um classificador com recall 100%. Como? E o que acontece com o precision?\n",
    "\n",
    "- Qual o precision e o recall do Never5Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "- É sempre possível construir um classificador com recall 100%. Como? E o que acontece com o precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basta categorizar todas as amostras como positivas - nunca teremos um *false negative* posto que nunca teremos um *negative*! Ou seja:\n",
    "\n",
    "- $FN = 0$ pois não teremos negativos\n",
    "\n",
    "- $TP = P$, o número completo de positivos reais da base, pois todos estes serão categorizados como positivos.\n",
    "\n",
    "Neste caso teremos \n",
    "\n",
    "$$\n",
    "\\text{recall} = \\frac{TP}{TP + FN} = \\frac{P}{P} = 1\n",
    "$$\n",
    "\n",
    "Neste caso o *precision* será afetado pelo alto número de *false positives*, pois todos os negativos serão classificados como positivos. Ou seja: $FP = N$. O *precision* então será:\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{TP}{TP + FP} = \\frac{P}{P + N}\n",
    "$$\n",
    "\n",
    "Em palavras: quando o *recall* é $100 \\%$, o *precision* é a fração de amostras positivas da base de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Qual o precision e o recall do Never5Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso o recall será zero, pois:\n",
    "\n",
    "- $TP = 0$ já que nenhuma amostra será categorizada como positiva\n",
    "\n",
    "- $FN = P$ pois todas as amostras verdadeiramente positivas serão classificadas como negativas\n",
    "\n",
    "Consequentemente:\n",
    "\n",
    "$$\n",
    "\\text{recall} = \\frac{TP}{TP + FN} = \\frac{0}{0 + P} = 0\n",
    "$$\n",
    "\n",
    "E como fica o *precision*? Na situação presente temos $FP = 0$ pois não teremos nenhuma amostra classificada como positiva, logo nenhuma amostra negativa será erroneamente classificada como positiva. O *precision* fica então:\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{TP}{TP + FP} = \\frac{0}{0 + 0} = \\text{indefinido}\n",
    "$$\n",
    "\n",
    "Contudo, podemos imaginar que o ``Never5Classifier`` é o caso limite de um classificador que quase nunca classifica uma amostra como positiva, a não ser que seja o $5$ mais ideal do universo (um ``AlmostNever5Classifier``). Suponha que nosso conjunto de teste é grande o suficiente para que tenhamos a presença de algumas cópias do $5$ ideal resultando em um $TP \\neq 0$, mas que $FP$ continue sendo zero. Neste caso:\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{TP}{TP + FP} = \\frac{TP}{TP + 0} = 1\n",
    "$$\n",
    "\n",
    "Portanto o ``AlmostNever5Classifier`` tem *recall* zero e *precision* $100 \\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular o precision e o recall no scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(precision_score(y_train_5, y_train_pred))\n",
    "print(recall_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Confirme se o scikit-learn acertou baseado na matriz de confusão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matriz de confusão era:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "53892 & 687 \\\\\n",
    "1891 & 3530\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Ou seja:\n",
    "\n",
    "- $TP = 3530$\n",
    "- $FP = 687$\n",
    "- $TN = 53892$\n",
    "- $FN = 1891$\n",
    "\n",
    "O *precision* será:\n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{TP}{TP + FP} = \\frac{3530}{3530 + 687} = 83,7 \\%\n",
    "$$\n",
    "\n",
    "O *recall* será:\n",
    "\n",
    "$$\n",
    "\\text{recall} = \\frac{TP}{TP + FN} = \\frac{3530}{3530 + 1891} = 65,1 \\%\n",
    "$$\n",
    "\n",
    "Estes resultados coincidem com aquele fornecido pelo `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A métrica $F_1$ serve para combinar o precision e o recall em uma métrica única que valoriza o equilibrio entre estas duas medidas. É definida como a média harmônica do precision e do recall:\n",
    "\n",
    "\n",
    "$$F_1 = \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}}$$\n",
    "\n",
    "No scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O valor de $F_1$ tende a favorecer precision e recall balanceados. Isto não é necessariamente bom, existem situações em que você quer favorecer um ou outro.\n",
    "\n",
    "**Perguntas:**\n",
    "\n",
    "- Dê um exemplo de situação onde precision é melhor que recall.\n",
    "\n",
    "- Dê um exemplo de situação onde recall é melhor que precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "- Dê um exemplo de situação onde precision é melhor que recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando vou comprar frutas, prefiro trazer menos frutas mas garantir que aquelas que eu pego são frescas.\n",
    "\n",
    "Melhor deixar frutas boas para trás do que correr o risco de pegar uma fruta podre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dê um exemplo de situação onde recall é melhor que precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eu tenho uma fábrica de gnomos de jardim. Por um problema de produção, alguns deles explodem se deixados no sol. Melhor fazer um *recall* dos meus gnomos: vou classificá-los como explosivos mesmo que muitos deles não sejam, melhor ser cuidadoso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/recall tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seria muito bom se tivéssemos um classificador com precision 100% e recall 100%, seria um classificador perfeito!\n",
    "\n",
    "Infelizmente o mundo real não é assim: quanto maior o precision menor o recall, e vice versa. Para entender isso melhor temos que conhecer um pouco mais a fundo como nosso classificador (SGD) funciona.\n",
    "\n",
    "Dentro do SGDClassifier, o primeiro passo da predição é calcular um valor para a amostra sobre a qual estamos fazendo a predição. Veremos em aulas subsequentes como isso funciona. Quanto maior o valor, mais provável é que a amostra seja positiva. Esta função que se aplica inicialmente chama-se \"função de decisão\" (decision function). \n",
    "\n",
    "Em seguida, usamos um parâmetro do classificador chamado de valor de limiar (threshold). Se o valor da função de decisão estiver acima do threshold, a amostra é classificada como positiva. Caso contrário, será classificada como negativa. \n",
    "\n",
    "<center>\n",
    "<img src=\"decision_function.png\" alt=\"função de decisão\" style=\"width: 800px;\"/>\n",
    "</center>\n",
    "\n",
    "**Perguntas:**\n",
    "\n",
    "- O que acontece se o threshold for muito, muito baixo? Como ficam os valores de precision e recall?\n",
    "\n",
    "- O que acontece se o threshold for muito, muito alto? Como ficam os valores de precision e recall?\n",
    "\n",
    "- Prove que se o precision e o recall são $100\\%$ temos um classificador que não erra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "- O que acontece se o threshold for muito, muito baixo? Como ficam os valores de precision e recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o threshold for muito baixo teremos muitas amostras sendo classificadas como positivas, sendo muitas delas verdadeiramente negativas. Consequentemente o precision será baixo. Já o recall será alto, pois poucas amostras verdadeiramente positivas serão deixadas de fora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O que acontece se o threshold for muito, muito alto? Como ficam os valores de precision e recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o threshold for muito alto o classificador é muito seletivo, e o precision será alto. Mas muitas amostras verdadeiramente positivas vão ficar de fora, diminuido o recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prove que se o precision e o recall são $100\\%$ temos um classificador que não erra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{precision} = \\frac{TP}{TP + FP} = 1 \\Longrightarrow TP = TP + FP \\Longrightarrow FP = 0\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{recall} = \\frac{TP}{TP + FN} = 1 \\Longrightarrow TP = TP + FN \\Longrightarrow FN = 0\n",
    "$$\n",
    "\n",
    "Logo o número de erros $FP + FN$ é zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar os valores da função de decisão calculados para nossas amostras de treinamento usando o scikit-learn: basta adicionar um parâmetro extra à chamada de `cross_val_predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(\n",
    "    sgd_clf,\n",
    "    X_train,\n",
    "    y_train_5,\n",
    "    cv=3,\n",
    "    method=\"decision_function\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora plotar os valores de precision e recall juntos em uma curva única:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Threshold\", fontsize=16)\n",
    "plt.legend(loc=\"upper left\", fontsize=16)\n",
    "plt.xlim([-40000, 40000])\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que essa curva significa? Significa que podemos ter qualquer valor de precision que quisermos, mas isso mexe no recall, e vice versa! \n",
    "\n",
    "Por exemplo: suponha que queremos um precision de 90% - queremos que nosso classificador esteja muito seguro de que achou um dígito 5. Como já temos os valores da função de decisão (`y_scores`), basta aplicar um threshold alto para ter um classificador de alta precisão! Olhando na curva acima, vemos que um threshold de aproximadamente 8000 deve servir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores > 8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos medir o precision e o recall deste classificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: {}'.format(precision_score(y_train_5, y_train_pred_90)))\n",
    "print('Recall: {}'.format(recall_score(y_train_5, y_train_pred_90)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentamos o precision para aproximadamente 90%, mas o recall caiu.\n",
    "\n",
    "Se estivermos interessados em observar apenas o compromisso entre precision e recall, podemos diagramar um contra o outro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Recall\", fontsize=16)\n",
    "plt.ylabel(\"Precision\", fontsize=16)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que após recall de 80% o precision cai muito rápido.\n",
    "\n",
    "**Pergunta:** Alguem chega para você e diz \"Meu classificador é o melhor! Tem precision de 99%!\". O que você pergunta em seguida?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "- E qual o recall? E qual a proporção de cada classe no conjunto de treinamento e de teste?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensibilidade, especificidade e curva ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra ferramenta útil para descrever o desempenho de um classificador binário é a curva de Característica de Operação do Receptor, mais conhecida pelo seu nome em inglês: *Receiver Operating Characterístic (ROC) curve*. É uma curva similar á curva precision-recall, mas usa a razão de falsos positivos (*False Positive Rate - FPR*) no eixo das abscissas, e a razão de positivos verdadeiros (*True Positive Rate - TPR*) no eixo das ordenadas. \n",
    "\n",
    "*True Positive Rate* é o mesmo que recall: a fração dos verdadeiros positivos que foram identificados como positivos pelo classificador. Outro nome para esta quantidade é sensibilidade (*sensitivity*). \n",
    "\n",
    "$$\\text{TPR} = \\text{recall} = \\text{sensitivity} = \\frac{\\text{TP}}{\\text{P}} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "\n",
    "Em termos simples, *sensitivity* é \"de todos os positivos, quantos eu detectei?\"\n",
    "\n",
    "O termo sensibilidade é muito usado na Medicina, em vista da importância da sensitividade no diagnóstico médico.\n",
    "\n",
    "**Pergunta:** No contexto de um programa de *screening* para detecção precoce do câncer de mama usando mamografias, o que significa alta sensitividade?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "Significa que casos de câncer de mama não serão ignorados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*False Positive Rate* é a fração dos negativos verdadeiros que foram identificados como positivos pelo classificador. \n",
    "\n",
    "$$\\text{FPR} = \\frac{FP}{N} = \\frac{FP}{TN + FP}$$\n",
    "\n",
    "Para entender o FPR, vamos entender outra quantidade que é o *True Negative Rate* (TNR), também conhecida como especificidade (*specificity*). A especificidade é a fração de negativos verdadeiros que foram identificados como negativos pelo classificador.\n",
    "\n",
    "$$\\text{TPR} = \\text{specificity} = \\frac{TN}{N} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "Em termos simples, *specificity* é \"de todos os negativos, quantos eu corretamente percebi como negativos?\"\n",
    "\n",
    "Combinando as expressões de FPR e TPR, vemos que:\n",
    "\n",
    "$$\\text{FPR} = 1 - \\text{specificity}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perguntas:** \n",
    "\n",
    "- Na questão anterior vimos que um programa de *screening* para câncer de mama deve ter alta sensitividade, para não ignorar mulheres (apesar de que homem tem câncer de mama também!) que estejam desenvolvendo a doença. Porém, com grande sensitividade vem baixa especificidade! O que acontece se for recomendado mastectomia em todos os casos detectados pelo *screening*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitas mastectomias inúteis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalmente, após uma detecção via *screening* a mulher é encaminhada para exames posteriores, incluindo biópsia. Em termos de sensibilidade e especificidade, o que esperamos de uma biópsia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alta sensibilidade e alta especificidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A curva ROC representa todos os pares (FPR, TPR) de um classificador binário que trabalhe com função de decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando escolhemos um valor específico de threshold para nosso classificador fixamos o ponto de trabalho deste na curva ROC. \n",
    "\n",
    "**Pergunta:** Um classificador perfeito opera em qual ponto do espaço $(\\text{FPR}, \\text{TPR})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "$(\\text{FPR}, \\text{TPR}) = (0.0, 1.0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que acontece com um classificador aleatório (decide ao acaso o valor da função de decisão)? Este classificador terá uma curva ROC como a linha tracejada acima. Qualquer classificador melhor que aleatório terá uma curva ROC acima da linha tracejada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos trocar de classificador e ver o que acontece:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# O \"score\" vai ser a probabilidade de que a amostra seja da classe positiva.\n",
    "y_probas_forest = cross_val_predict(\n",
    "    forest_clf,\n",
    "    X_train,\n",
    "    y_train_5,\n",
    "    cv=3,\n",
    "    method=\"predict_proba\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Gambiarra para desviar do bug #9589 introduzido no Scikit-Learn 0.19.0:\n",
    "y_scores_forest = y_probas_forest[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(\n",
    "    y_train_5,\n",
    "    y_scores_forest,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\n",
    "plt.plot(fpr_forest, tpr_forest, linewidth=2, label=\"Random Forest\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que o classificador RandomForest apresenta TPR mais alto para um mesmo FPR do que o SGD. Com isso, parece que o RandomForest tem melhor desempenho que o SGD.\n",
    "\n",
    "Para sumarizar o desempenho de um classificador binário em apenas um número, usamos a área sob a curva ROC (ROC AUC - Area Under the Curve - score). \n",
    "\n",
    "**Perguntas:** \n",
    "\n",
    "- Qual o valor do ROC AUC score para o classificador aleatório?\n",
    "\n",
    "- Qual o valor do ROC AUC score para um classificador perfeito?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "- Qual o valor do ROC AUC score para o classificador aleatório?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Qual o valor do ROC AUC score para um classificador perfeito?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pergunta:**\n",
    "\n",
    "O que você prefere ter: um classificador com $AUC = 0.75$ ou com $AUC = 0.1$? Justifique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R**:\n",
    "\n",
    "O classificador com $AUC = 0.1$ é preferível, desde que qualquer categorização fornecida por esse classificador seja ignorada e a classe oposta seja adotada como a classe correta. Neste caso convertemos o classificador para um novo classificador derivado, com $AUC = 0.9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos então comparar os escores ROC AUC dos dois classificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('SGD: {:4f}'.format(roc_auc_score(y_train_5, y_scores)))\n",
    "print('RandomForest: {:4f}'.format(roc_auc_score(y_train_5, y_scores_forest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest é um classificador melhor do que SGD neste problema. Veja também em termos da curva precision-recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(\n",
    "    y_train_5, y_scores_forest)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(precisions, recalls, label='SGD')\n",
    "plt.plot(precisions_forest, recalls_forest, label='Random Forest')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De fato, o classificador RandomForest é superior ao classificador SGD em qualquer valor de precisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação multiclasse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um classificador cinco-ou-não-cinco pode até ser útil, mas o problema real que queremos resolver é descobrir qual o dígito a partir de sua imagem. **Classificadores multiclasse** servem para este propósito.\n",
    "\n",
    "Alguns algoritmos (como Random Forest) são intrinsecamente capazes de fazer classificação multi-classes. Outros são unicamente classificadores binários, como o SGD. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.fit(X_train, y_train)  # Aqui estamos usando y_train, não y_train_5!\n",
    "forest_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf.predict_proba([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo RandomForest estima uma probabilidade de $90\\%$ de que o dígito em questão seja um $5$, $1\\%$ de que seja um $2$, $8\\%$ de que seja um $3$ e $1\\%$ de que seja um $9$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-versus-One e One-versus-All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma de fazer um classificador multi-classe à partir de um classificador exclusivamente binário é construir um classificador binário para cada classe, e comparar os scores resultantes de cada um. Como cada classificador é do tipo um-versus-outros, esta abordagem é conhecida como estratégia *one-versus-all* (OvA).\n",
    "\n",
    "Outra possibilidade é treinar um conjunto de classificadores binários comparando classe-versus-classe. Por exemplo: no caso dos dígitos, teríamos o classificador zero-versus-um, zero-versus-dois, etc, até o classificador oito-versus-nove, para um total de 45 classificadores. Em geral, para um problema de $N$ classes temos $N \\cdot (N - 1) / 2$ classificadores. Temos muito mais classificadores, mas cada um deles é treinado para resolver um problema muito mais específico (e.g. distinguir 5 de 7, ao invés de ter que distinguir 5 do resto). Ademais, o conjunto de treinamento de cada um destes classificadores especializados é muito menor.\n",
    "\n",
    "Esta estratégia é conhecida como *one-versus-one* (OvO).\n",
    "\n",
    "Quando usar qual deles (OvO versus OvA)? Alguns classificadores escalam mal com o número de amostras: nestes casos OvO é preferível. Em outros casos a simplicidade do OvA é melhor.\n",
    "\n",
    "Scikit-learn usa automaticamente OvA para seus classificadores binários que não possam ser multiclasse automaticamente - exceto para Support Vector Machines, para os quais OvO é preferível por questão de escalabilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(\n",
    "    max_iter=500,\n",
    "    tol=1e-3,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    ")  # Existe aleatoriedade dentro do SGDClassifier.\n",
    "\n",
    "sgd_clf.fit(X_train, y_train)  # Aqui estamos usando y_train, não y_train_5!\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para confirmar esse resultado, observe os valores da função de decisão para cada uma das classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = sgd_clf.decision_function([some_digit])\n",
    "print(sgd_clf.classes_)\n",
    "print(some_digit_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o maior score corresponde à classe $5$ realmente.\n",
    "\n",
    "**ATENÇÃO**: neste exercício temos uma coincidência: \n",
    "\n",
    "- A classe $0$ corresponde ao índice 0 no vetor de classes.\n",
    "- A classe $1$ corresponde ao índice 1 no vetor de classes.\n",
    "- ... e assim por diante.\n",
    "\n",
    "Trata-se de uma coincidência! Se as classes fossem \"cadeira\", \"mesa\" e \"chapeu\" essa coincidência não existiria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você tem um classificador binário qualquer (você fez o seu próprio classificador, por exemplo) e quer usá-lo em classificação multiclasse, o scikit-learn já tem classes auxiliares para transformar seu classificador binário em OvO ou OvA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "ovo_clf = OneVsOneClassifier(\n",
    "    SGDClassifier(\n",
    "        max_iter=5,\n",
    "        tol=-np.infty,\n",
    "        random_state=RANDOM_SEED,\n",
    "    ))\n",
    "ovo_clf.fit(X_train, y_train)\n",
    "ovo_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ovo_clf.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de desempenho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como no caso dos classificadores binários, estamos interessados em estimar a performance real dos nossos classificadores multiclasse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(\n",
    "    sgd_clf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nada mal: um classificador aleatório teria apenas um desempenho médio de 10% de acurácia!\n",
    "\n",
    "Podemos melhorar o desempenho do classificador usando todos os truques das aulas passadas (GridSearch, scaling, data augmentation, etc). Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(\n",
    "    sgd_clf,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom, você já usou todos os truques básicos do nosso arsenal, e agora tem um modelo que é o melhor que você conseguiu até o momento. Para avançar mais, temos que mergulhar mais a fundo na análise dos erros que nosso classificador está fazendo.\n",
    "\n",
    "Uma primeira abordagem para a análise fina dos erros é a matriz de confusão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(\n",
    "    sgd_clf,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma visualização gráfica pode ser mais efetiva:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(np.log(1 + conf_mx), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esperado, a maior parte das predições é correta, e portanto a diagonal da matriz de confusão se sobressai. Para ressaltar os erros, vamos fazer o seguinte:\n",
    "\n",
    "- Normalizar as linhas pela soma dos valores da linha.\n",
    "- Zerar os elementos da diagonal, para facilitar a visualização dos erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "\n",
    "plt.matshow(np.log(1 + norm_conf_mx), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora está mais claro: parece que temos muitos $5$ que são classificados como $3$ e vice-versa! Temos também vários $5$ classificados como $8$, mas o reverso é menos presente.\n",
    "\n",
    "Pode ser ilustrativo observar alguns exemplos específicos de erro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_a, cl_b = 3, 5\n",
    "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(221)\n",
    "plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.subplot(222)\n",
    "plot_digits(X_ab[:25], images_per_row=5)\n",
    "plt.subplot(223)\n",
    "plot_digits(X_ba[:25], images_per_row=5)\n",
    "plt.subplot(224)\n",
    "plot_digits(X_bb[:25], images_per_row=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns erros são compreensíveis (digitos que de fato se parecem tanto com 3 como com 5), outros exemplos são misteriosos, como alguns digitos que são claramente 5 e foram classificados como 3. Por que um dígito que tão claramente se parece com um 5 foi classificado como 3? \n",
    "\n",
    "A resposta está no tipo de classificador usado. O SGDClassifier é um modelo linear: uma mera ponderação linear dos valores dos píxels. Se dois dígitos diferem apenas por poucos pixels, é fácil confundí-los."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade**: Agora que você tem um modelo treinado, avalie a acurária do modelo no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividades:** Faça os problemas 1 e 2 do capítulo 3 do livro texto (Géron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desafio:** Problema 4 do livro texto (Géron)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
